{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rajaeyassine/miniconda3/envs/rag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the model and tokenizer from Hugging Face\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"  # Example model for embeddings\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.86758161e-01  1.60997123e-01 -2.67068427e-02  7.02959299e-02\n",
      " -1.97098866e-01 -5.70063174e-01  1.55012146e-01  9.35192872e-03\n",
      " -2.08503664e-01  1.42686889e-01  2.07546368e-01  1.56744406e-01\n",
      "  1.45996884e-01  5.37145250e-02 -2.63866663e-01 -2.45950162e-01\n",
      " -1.14997894e-01  5.27315252e-02 -6.73651159e-01  2.01303456e-02\n",
      "  1.43493935e-01  3.26979131e-01 -7.52834156e-02  2.36951232e-01\n",
      " -4.31217819e-01 -6.22581542e-02  2.02568009e-01  1.99742869e-01\n",
      " -2.45356843e-01 -2.84341007e-01  2.38731384e-01  3.37002307e-01\n",
      "  2.87383080e-01  4.27109487e-02 -7.78701380e-02  4.16504472e-01\n",
      " -3.82288069e-01 -3.79202962e-01  1.01378419e-01  7.92204812e-02\n",
      "  1.59037784e-01 -2.61661261e-01 -3.04415822e-01 -1.18944325e-01\n",
      "  3.62622924e-02  1.17296197e-01  3.11168134e-02  2.50195861e-01\n",
      "  3.55965883e-01  1.69502035e-01 -2.67933786e-01 -2.90228158e-01\n",
      " -3.50596346e-02  9.85115469e-02  1.75356194e-01  2.73501631e-02\n",
      "  5.27028181e-02 -2.58113950e-01  4.96397167e-02 -4.27142717e-02\n",
      " -3.07286292e-01  1.88174486e-01 -6.82062134e-02  3.59680086e-01\n",
      "  4.40310031e-01 -3.91256720e-01 -8.02002922e-02  2.18776703e-01\n",
      " -3.36714268e-01 -1.61469221e-01 -7.50962123e-02  5.52767813e-02\n",
      "  1.78545311e-01  3.24168473e-01 -2.66151041e-01  4.30250466e-02\n",
      "  5.90671413e-02 -1.86721519e-01  3.35587524e-02  2.50090390e-01\n",
      "  3.78717750e-01 -6.01518631e-01  7.99974799e-02  2.42357388e-01\n",
      "  1.55295208e-01 -1.94054842e-01  8.36098194e-03  4.73019667e-02\n",
      " -1.59132436e-01 -1.65979370e-01 -6.51915014e-01  3.61856110e-02\n",
      " -5.00650257e-02  1.88785151e-01 -4.56461221e-01 -2.03766465e-01\n",
      "  3.41789633e-01 -1.28600970e-01 -7.32306480e-01  6.57239735e-01\n",
      "  1.83459267e-01  2.58675545e-01  2.19974041e-01  9.09864977e-02\n",
      "  2.66241640e-01  8.44907761e-02 -1.59075603e-01  2.25418881e-01\n",
      " -2.28635386e-01 -1.49738804e-01 -8.91640782e-02 -2.38214001e-01\n",
      "  1.60737485e-01 -1.91811677e-02  2.45110810e-01 -2.84929991e-01\n",
      " -4.93147559e-02  5.16108274e-02 -1.96439207e-01 -7.60210305e-03\n",
      "  2.97802240e-01 -2.22583547e-01  2.40973637e-01  1.27796963e-01\n",
      "  9.71065387e-02 -7.75599480e-03  2.91421384e-01 -3.18870446e-32\n",
      "  3.11051369e-01  1.50086777e-02  1.41299590e-01  8.48202229e-01\n",
      "  1.46504939e-02  1.35454968e-01 -4.65454251e-01 -1.52435198e-01\n",
      "  1.30465806e-01 -5.31812347e-02  1.16990246e-01  1.16644077e-01\n",
      " -1.52707532e-01  2.41686508e-01 -1.22193180e-01  4.98112410e-01\n",
      " -3.87820512e-01 -1.58925224e-02  2.10577175e-01  4.64401215e-01\n",
      " -3.25670242e-01  3.09618767e-02  1.08985901e-01  3.42318296e-01\n",
      " -3.68379839e-02 -8.53711367e-03  1.32299513e-01 -3.68477553e-01\n",
      "  5.57957113e-01  4.18739766e-02 -1.15878977e-01 -2.29823232e-01\n",
      "  7.03058615e-02  9.69600901e-02 -2.24260483e-02  6.72257924e-03\n",
      " -1.67886391e-01 -2.64765441e-01 -4.60642010e-01 -1.41668007e-01\n",
      " -9.19437036e-02  2.24145994e-01  2.32799768e-01 -1.56291556e-02\n",
      " -1.62494361e-01 -6.54976591e-02  2.49640420e-01  1.52019188e-01\n",
      "  7.49166533e-02  2.65249103e-01 -4.16081101e-01  6.50841519e-02\n",
      " -2.33901426e-01  3.47438067e-01 -6.43769428e-02 -1.18825110e-02\n",
      "  2.45662645e-01 -2.03548476e-01 -6.93158880e-02  1.58439174e-01\n",
      "  2.62951851e-02  4.46267754e-01  2.22642664e-02 -8.97284225e-02\n",
      " -7.43712708e-02 -2.26848051e-01  1.89172938e-01  7.18376860e-02\n",
      "  2.54389822e-01  9.33857542e-03 -7.29743317e-02  1.32222608e-01\n",
      "  1.52981922e-01  1.15860403e-01 -2.35005915e-02  1.76899716e-01\n",
      "  3.26722652e-01 -9.24967695e-03  1.04560606e-01 -2.82225758e-01\n",
      "  9.37172174e-02  1.54383317e-01 -9.02408659e-02 -1.99447155e-01\n",
      "  5.08552074e-01  5.82498945e-02 -7.30331913e-02 -5.14031112e-01\n",
      " -6.05177879e-02 -1.48677826e-03 -4.64888126e-01  2.85203069e-01\n",
      "  2.08448127e-01 -1.22357763e-01 -4.62817043e-01  1.95842369e-32\n",
      "  6.46574736e-01  2.67015547e-02 -1.62172556e-01 -4.45543379e-01\n",
      " -1.54456005e-01 -1.65768579e-01 -3.52161139e-01  6.15946233e-01\n",
      " -4.07950401e-01  2.58184731e-01  5.51924109e-03  1.07513040e-01\n",
      "  5.09067357e-01  6.35396317e-02  1.99938893e-01  9.16280523e-02\n",
      "  5.59075415e-01  1.21638499e-01  7.14724287e-02  3.02649830e-02\n",
      " -5.56377582e-02 -1.74644753e-01 -1.86086103e-01  5.46660312e-02\n",
      " -2.50903904e-01  3.85810249e-02  3.28834683e-01  1.66520216e-02\n",
      " -4.54201430e-01  1.81236759e-01 -1.08789444e-01  1.96057931e-01\n",
      " -1.50245070e-01 -5.58719449e-02 -7.05345497e-02  1.22518413e-01\n",
      " -4.77238089e-01 -1.73151866e-01 -1.83548450e-01 -4.92534004e-02\n",
      " -3.12523812e-01  1.24608636e-01  1.00882389e-01  1.84311971e-01\n",
      " -5.10133684e-01 -1.38345420e-01 -2.54895687e-01  6.29038140e-02\n",
      " -2.51559228e-01 -1.42050311e-01 -4.71567243e-01 -2.07223102e-01\n",
      "  3.28048080e-01 -1.50526598e-01 -5.08482270e-02  1.34088039e-01\n",
      " -1.37023628e-01  5.03339581e-02  2.10798427e-01  1.09077573e-01\n",
      "  3.91906947e-02  2.74664968e-01  2.00009748e-01  4.54338461e-01\n",
      "  8.11317042e-02 -2.63304383e-01  2.80870008e-03  2.48430744e-01\n",
      "  2.07843974e-01 -1.42876700e-01  4.51959781e-02 -5.22137769e-02\n",
      " -1.81746587e-01  1.15790265e-02 -1.48447648e-01  3.64471108e-01\n",
      "  1.28642470e-02 -8.61419067e-02  1.37983933e-02  1.87722728e-01\n",
      "  3.53566743e-02  2.23440245e-01  1.95884541e-01  6.96981028e-02\n",
      " -7.00324699e-02  2.86936104e-01  1.77834764e-01  2.70201117e-01\n",
      " -9.77806821e-02 -3.93533021e-01 -1.47968426e-01 -7.29523897e-02\n",
      "  1.08936489e-01  5.85200004e-02 -3.38036209e-01 -9.20021321e-08\n",
      " -3.84260058e-01  2.28504106e-01 -1.17798872e-01  3.10332477e-01\n",
      "  1.17473036e-01  7.04191858e-03 -4.44356114e-01 -3.27148944e-01\n",
      " -3.91733289e-01  2.77195890e-02  2.62490600e-01  5.12993395e-01\n",
      " -3.27171564e-01  7.58166537e-02  3.28335673e-01  3.46770495e-01\n",
      " -1.56491354e-01  1.02159791e-01 -1.07320338e-01 -3.55585627e-02\n",
      " -5.28940558e-02  2.00078040e-02  1.62192747e-01 -3.86353105e-01\n",
      "  1.89390481e-01 -3.68457913e-01 -7.73152784e-02  2.91588604e-02\n",
      "  2.54328605e-02 -3.00509840e-01  2.05487773e-01  4.66503620e-01\n",
      " -2.11492881e-01  7.04111829e-02 -5.18967450e-01 -1.36944011e-01\n",
      "  5.36454655e-02  3.40136081e-01  3.27663034e-01 -3.65792990e-01\n",
      " -3.84377509e-01  2.09111288e-01 -1.69277728e-01 -5.18814862e-01\n",
      " -1.74314722e-01  2.51938075e-01  3.35943371e-01 -2.44469047e-01\n",
      "  7.47963190e-02 -3.15807074e-01 -3.71460199e-01  1.27943859e-01\n",
      "  3.63291949e-01 -6.09029531e-02  6.52077734e-01  3.65748316e-01\n",
      "  2.50720173e-01  1.02682345e-01 -1.31474182e-01  4.34918970e-01\n",
      "  1.95778415e-01 -2.00030670e-01  1.55911028e-01  8.88523832e-02]\n"
     ]
    }
   ],
   "source": [
    "# Load the model and tokenizer from Hugging Face\n",
    "\n",
    "\n",
    "def get_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Use the mean of the last hidden state as the sentence embedding\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "    return embeddings\n",
    "\n",
    "# Test the model\n",
    "test_text = \"Hello, world!\"\n",
    "embeddings = get_embeddings(test_text)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the review data\n",
    "with open(\"reviews.json\") as f:\n",
    "    data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = []\n",
    "\n",
    "# Create embeddings for each review\n",
    "for review in data[\"reviews\"]:\n",
    "    embedding = get_embeddings(review['review'])\n",
    "    processed_data.append(\n",
    "        {\n",
    "            \"values\": embedding.tolist(),\n",
    "            \"id\": review[\"professor\"],\n",
    "            \"metadata\": {\n",
    "                \"review\": review[\"review\"],\n",
    "                \"subject\": review[\"subject\"],\n",
    "                \"stars\": review[\"stars\"],\n",
    "            }\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder function to represent vector store insertion\n",
    "def insert_into_vector_store(vectors, namespace):\n",
    "    # Replace this function with actual vector store logic\n",
    "    print(f\"Inserted {len(vectors)} vectors into the vector store under namespace '{namespace}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 20 vectors into the vector store under namespace 'ns1'.\n",
      "Processed 20 reviews.\n"
     ]
    }
   ],
   "source": [
    "# Insert the embeddings into the vector store\n",
    "insert_into_vector_store(processed_data, namespace=\"ns1\")\n",
    "\n",
    "# Print a summary of inserted data\n",
    "print(f\"Processed {len(processed_data)} reviews.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
